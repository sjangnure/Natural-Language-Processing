{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_assignmnet2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1VseOYoEuy_2LMMCOEtNqJuCKHeNtZtXO",
      "authorship_tag": "ABX9TyMfvXWFZwuU863z1vUx5RBw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sjangnure/Natural-Language-Processing/blob/main/NLP_assignmnet2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYJKpzUZGrwx"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "Name: Sanjana Jangnure\n",
        "NetId: Sbj286\n",
        "NLP Assignment 2: Bag of words, tfidf, Naive Bayes\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpgsrKGqovYp"
      },
      "source": [
        "##Import the dataset using pandas, make a column for 'RealNews?' and append both the files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqh66L_Ym8F2",
        "outputId": "a5afb72d-85b2-49a1-e5f8-ec2f275358b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "df_real = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/True.csv')\n",
        "df_real['RealNews?'] = True\n",
        "df_fake = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/Fake.csv')\n",
        "df_fake['RealNews?'] = False\n",
        "df = df_real.append(df_fake)\n",
        "\n",
        "df=df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>RealNews?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
              "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>U.S. military to accept transgender recruits o...</td>\n",
              "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... RealNews?\n",
              "0  As U.S. budget fight looms, Republicans flip t...  ...      True\n",
              "1  U.S. military to accept transgender recruits o...  ...      True\n",
              "\n",
              "[2 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beGpSeP6ahFI"
      },
      "source": [
        "##Preprocessing of data- combining, removing duplicates and splitting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g74gI-dT5qIw",
        "outputId": "7ad7a9d9-dfde-4341-9356-eb960d35a931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Combining title and text column\n",
        "df['document'] = df[['title', 'text']].agg(' '.join, axis=1)\n",
        "df['document'] = df['document'].apply(lambda x: x.lower())\n",
        "\n",
        "#drop duplicates\n",
        "df=df.drop_duplicates()\n",
        "df = df.sample(2000)\n",
        "len(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW8M159O6kuO",
        "outputId": "80670c7d-cc61-4dcb-e332-30031ba79431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "# Split into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, shuffle=True)\n",
        "df_train.reset_index(drop=True)\n",
        "df_test.reset_index(drop=True)\n",
        "print(len(df_train))\n",
        "print(len(df_test))\n",
        "df_train.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1600\n",
            "400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>RealNews?</th>\n",
              "      <th>document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7327</th>\n",
              "      <td>Britain's Johnson says no need for gloom about...</td>\n",
              "      <td>BELGRADE (Reuters) - British ForeignÂ Secretary...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>November 10, 2016</td>\n",
              "      <td>True</td>\n",
              "      <td>britain's johnson says no need for gloom about...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24336</th>\n",
              "      <td>WATCH: Bill Maher Returns And Hilariously Moc...</td>\n",
              "      <td>Bill Maher returned from break just in time fo...</td>\n",
              "      <td>News</td>\n",
              "      <td>January 21, 2017</td>\n",
              "      <td>False</td>\n",
              "      <td>watch: bill maher returns and hilariously moc...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   title  ...                                           document\n",
              "7327   Britain's Johnson says no need for gloom about...  ...  britain's johnson says no need for gloom about...\n",
              "24336   WATCH: Bill Maher Returns And Hilariously Moc...  ...   watch: bill maher returns and hilariously moc...\n",
              "\n",
              "[2 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7LtR67Kalpu"
      },
      "source": [
        "##Q1. Naive Bayes by hand"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfyK3sTzaqlY"
      },
      "source": [
        "# Tokenize the document\n",
        "\n",
        "# Create a dictionary to store the bag of words\n",
        "vocab=dict()\n",
        "\n",
        "for row in df_train['document']:\n",
        "  dictionary=re.split(r\"\\W+\",row)\n",
        "  for tokens in dictionary:\n",
        "    if tokens not in vocab.keys():\n",
        "      vocab[tokens]=1\n",
        "    else:\n",
        "      vocab[tokens]+=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNOFR0M5oTrs",
        "outputId": "30dea448-9e36-4a97-e54a-843f148786c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# Calculate priors\n",
        "# P(True)=No. of True docs in train/Total no. of docs in Train\n",
        "true_lest=len(df_train[df_train['RealNews?']])\n",
        "false_test=len(df_train[~df_train['RealNews?']])\n",
        "print(\"No. of documents in Train is :\",len(df_train))\n",
        "print(\"No. of True documents in Train is: \",true_lest)\n",
        "print(\"No. of False documents in Train is: \",false_test)\n",
        "P_true=true_lest/len(df_train)\n",
        "P_false=false_test/len(df_train)\n",
        "print(\"P(True)=\",P_true)\n",
        "print(\"P(False)=\",P_false)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of documents in Train is : 1600\n",
            "No. of True documents in Train is:  771\n",
            "No. of False documents in Train is:  829\n",
            "P(True)= 0.481875\n",
            "P(False)= 0.518125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayenhG95vP2h"
      },
      "source": [
        "Conditional probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6OleWixqNNu"
      },
      "source": [
        "dictionary=[]\n",
        "vocab_true=dict()\n",
        "for row in df_train[df_train['RealNews?']]['document']:\n",
        "  dictionary=re.split(r\"\\W+\",row)\n",
        "  for tokens in dictionary:\n",
        "    if tokens not in vocab_true.keys():\n",
        "      vocab_true[tokens]=1\n",
        "    else:\n",
        "      vocab_true[tokens]+=1\n",
        "\n",
        "# print(vocab_true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa-Kqx3Cw4AX"
      },
      "source": [
        "dictionary=[]\n",
        "vocab_false=dict()\n",
        "for row in df_train[~df_train['RealNews?']]['document']:\n",
        "  dictionary=re.split(r\"\\W+\",row)\n",
        "  for tokens in dictionary:\n",
        "    if tokens not in vocab_false.keys():\n",
        "      vocab_false[tokens]=1\n",
        "    else:\n",
        "      vocab_false[tokens]+=1\n",
        "\n",
        "# print(vocab_false)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Kt7GZSOyXRu",
        "outputId": "0ba7d294-6633-4b53-d3b1-727a15d539c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Find |V|\n",
        "# Calculate the count of unique tokens in the vocbulary \n",
        "V=len(vocab)\n",
        "print(\"|V|=\",V)\n",
        "\n",
        "#count(c) = total tokens in the documents with class c\n",
        "# count_true= count(c=True)=total tokens in the documents with class true\n",
        "count_true=sum(vocab_true.values())\n",
        "count_false=sum(vocab_false.values())\n",
        "print(count_true,count_false)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|V|= 28742\n",
            "303397 390435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS5U7rTheiIG",
        "outputId": "21a4e17f-3683-4692-d643-7c0d07736840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Testing the data\n",
        "dictionary=[]\n",
        "predictions=[]\n",
        "row=\"\"\n",
        "vocab2=dict()\n",
        "\n",
        "for row in df_test['document']:\n",
        "  # row=df_test['document'][i]\n",
        "  # print(type(row))\n",
        "  dictionary=re.split(r\"\\W+\",row)\n",
        "  value1=P_true\n",
        "  value2=P_false\n",
        "  for tokens in dictionary:\n",
        "    if tokens in vocab_true:\n",
        "      val1=(vocab_true[tokens]+1)/(count_true+V)\n",
        "    else:\n",
        "      val1=1/(count_true+V)\n",
        "    if tokens in vocab_false:\n",
        "      val2=(vocab_false[tokens]+1)/(count_false+V)\n",
        "    else:\n",
        "      val2=1/(count_false+V)\n",
        "    vocab2[tokens]=(val1,val2)\n",
        "    value1=value1*vocab2[tokens][0]\n",
        "    value2=value2*vocab2[tokens][1]\n",
        "  if value1>value2:\n",
        "    predictions.append(True)\n",
        "  else:\n",
        "    predictions.append(False)\n",
        "\n",
        "print(len(predictions))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWSpUyMeejnj",
        "outputId": "73d45c2c-e394-41e3-e2a5-a19a9f5e0620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "precision_recall_fscore_support(df_test['RealNews?'], predictions,average='macro')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7751988810210683, 0.5792069243156199, 0.5070562512422977, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXGtl1XFsYGp"
      },
      "source": [
        "##Q2. tf-idf by hand"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCNdl2TwrjMo",
        "outputId": "6ed0432e-a2d2-49da-c61a-3b480980958c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "vocab_train=dict()\n",
        "i = 0\n",
        "for index, row in df_train.iterrows():\n",
        "    for word in row['document']:\n",
        "        if(word in vocab_train):\n",
        "            vocab_train[word] += 1\n",
        "        else:\n",
        "            vocab_train[word] = 1\n",
        "    \n",
        "# Selecting tokens with frequency >=2 \n",
        "\n",
        "BagOfWords = dict()\n",
        "for tokens in vocab_train:\n",
        "    if(vocab_train[tokens]>=2):\n",
        "        BagOfWords[tokens] = vocab_train[tokens]\n",
        "        \n",
        "print('Bag of words before limiting min count to 2:',len(vocab_train))\n",
        "print('Bag of Words length: ',len(BagOfWords))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bag of words before limiting min count to 2: 80\n",
            "Bag of Words length:  75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOrHL_ehsE20"
      },
      "source": [
        "import math\n",
        "IDF = dict()\n",
        "\n",
        "\n",
        "for index, row in df_train.iterrows():\n",
        "    for tokens in set(row['document']):\n",
        "        if tokens in BagOfWords:\n",
        "            if tokens in IDF:\n",
        "                IDF[tokens] += 1\n",
        "            else:\n",
        "                IDF[tokens] = 1\n",
        "    \n",
        "\n",
        "for tokens in IDF:\n",
        "    IDF[tokens] = math.log(len(df_train)/IDF[tokens],10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh5q1SyytHjP"
      },
      "source": [
        "BagOfWordsList = list(BagOfWords.keys())\n",
        "def word_vectorizer(words):\n",
        "    res = [0]*len(BagOfWords)\n",
        "    for word in words:\n",
        "        if(word in BagOfWordsList):\n",
        "            wordIDF = IDF[word]\n",
        "            wordTF = 1 + math.log(words.count(word),10)\n",
        "            res[BagOfWordsList.index(word)] = wordTF * wordIDF\n",
        "    return(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BVfrr9otL1g"
      },
      "source": [
        "# Vectorizing the train dataset\n",
        "# print('Vectorizing Training set')\n",
        "i = 0\n",
        "\n",
        "trainData = []\n",
        "trainLabels = []\n",
        "\n",
        "for index, row in df_train.iterrows():\n",
        "    trainData.append(word_vectorizer(row['document']))\n",
        "    trainLabels.append(1 if row['RealNews?'] else 0)\n",
        "    i += 1\n",
        "\n",
        "# print(\"Training set Vectorized\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcffXZI5tQOM"
      },
      "source": [
        "# vectorizing test dataset\n",
        "\n",
        "testData = []\n",
        "testLabels = []\n",
        "\n",
        "for index, row in df_test.iterrows():\n",
        "    testData.append(word_vectorizer(row['document']))\n",
        "    testLabels.append(1 if row['RealNews?'] else 0)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ERzmM5dtePm",
        "outputId": "70c37e99-c044-4bba-eaf1-f9a7093d3657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf2 = LogisticRegression(random_state=0)\n",
        "clf2.fit(trainData, trainLabels)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwnu86mvtnNP"
      },
      "source": [
        "predictions2 = clf2.predict(testData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN-1_avuyWMg",
        "outputId": "c4526cbc-1143-493f-f78d-d401d3e2c2b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "precision_recall_fscore_support(df_test['RealNews?'],predictions2,average='macro')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9441997798018216, 0.946658615136876, 0.9448331201885705, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTMTr7Im0nmx"
      },
      "source": [
        "##Q3 a) Naive Bayes using scikitlearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2noJLs4Wf3t",
        "outputId": "bc231e7c-19ab-4ab0-887c-208c39b61352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "cv = CountVectorizer(strip_accents='ascii', token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b', lowercase=True, stop_words='english',min_df=2)\n",
        "X_train_cv = cv.fit_transform(df_train['document'])\n",
        "X_test_cv = cv.transform(df_test['document'])\n",
        "\n",
        "naive_bayes = MultinomialNB()\n",
        "naive_bayes.fit(X_train_cv,df_train['RealNews?'])\n",
        "predictions = naive_bayes.predict(X_test_cv)\n",
        "\n",
        "precision_recall_fscore_support(df_test['RealNews?'],predictions,average='macro')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9298820445609437, 0.9291465378421899, 0.9294905693636524, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS3SfX_aT_KI"
      },
      "source": [
        "##Q3 b) TF-IDF using scikitlearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLxW-R5Z0xNy",
        "outputId": "987d50d9-6660-40a4-d143-36e09d916978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "text_transformer = TfidfVectorizer(stop_words='english',ngram_range=(1,2),analyzer='word',lowercase=True,min_df=2)\n",
        "X_train_text = text_transformer.fit_transform(df_train['document'])\n",
        "X_test_text = text_transformer.transform(df_test['document'])\n",
        "\n",
        "logit = LogisticRegression(C=5e1, solver='lbfgs', random_state=17, n_jobs=4)\n",
        "logit.fit(X_train_text,df_train['RealNews?'])\n",
        "\n",
        "test_preds = logit.predict(X_test_text)\n",
        "\n",
        "precision_recall_fscore_support(df_test['RealNews?'],test_preds,average='macro')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9641426282051282, 0.9663848631239935, 0.9648735447611401, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EImx5epwHt4s"
      },
      "source": [
        "\n",
        "\n",
        "1.   **character ngram analyzer**: more the ngram more is the precision and recall. But the computation time increases as well. Did not change the scores much from (1,2) to (1,3) but a significant difference from (1,1) to (1,2)\n",
        "2.   **stop words**: We can add manual list of stop words to make the model better. Removing or adding stop words parameter did not make much difference to the scores\n",
        "3. **min_df**: value of 2 makes the computation faster.\n",
        "\n"
      ]
    }
  ]
}